{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c1492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8160237",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('reddit_questions.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60124e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>izucgz</td>\n",
       "      <td>What's the purpose of life?</td>\n",
       "      <td>8</td>\n",
       "      <td>1.601076e+09</td>\n",
       "      <td>Fri Sep 25 23:13:31 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9c784/</td>\n",
       "      <td>I've tried to quit smoking, this is my seventh...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.250712e+09</td>\n",
       "      <td>Wed Aug 19 19:58:54 2009 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iylxwl</td>\n",
       "      <td>For those who have a slave master last name, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.600904e+09</td>\n",
       "      <td>Wed Sep 23 23:35:15 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gmmlj4</td>\n",
       "      <td>How do you think humans will become extinct?</td>\n",
       "      <td>21998</td>\n",
       "      <td>1.589887e+09</td>\n",
       "      <td>Tue May 19 11:18:05 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ishb7v</td>\n",
       "      <td>What is a movie So Disturbing you couldn't be ...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.600074e+09</td>\n",
       "      <td>Mon Sep 14 08:53:53 2020 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  votes  \\\n",
       "0  izucgz                        What's the purpose of life?      8   \n",
       "1  9c784/  I've tried to quit smoking, this is my seventh...     11   \n",
       "2  iylxwl  For those who have a slave master last name, w...      0   \n",
       "3  gmmlj4       How do you think humans will become extinct?  21998   \n",
       "4  ishb7v  What is a movie So Disturbing you couldn't be ...     13   \n",
       "\n",
       "      timestamp                      datetime  \n",
       "0  1.601076e+09  Fri Sep 25 23:13:31 2020 UTC  \n",
       "1  1.250712e+09  Wed Aug 19 19:58:54 2009 UTC  \n",
       "2  1.600904e+09  Wed Sep 23 23:35:15 2020 UTC  \n",
       "3  1.589887e+09  Tue May 19 11:18:05 2020 UTC  \n",
       "4  1.600074e+09  Mon Sep 14 08:53:53 2020 UTC  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a93f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv('reddit_answers.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff64bb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'q_id', 'text', 'votes'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be278fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0b0ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hvbvpz</td>\n",
       "      <td>Two pet ducks. You may be tempted to go for on...</td>\n",
       "      <td>2359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hvbvpz</td>\n",
       "      <td>Nice try Jeff Bezos</td>\n",
       "      <td>764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hvbvpz</td>\n",
       "      <td>A curved shower rod. Seriously. $10 for a tens...</td>\n",
       "      <td>1525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hvbvpz</td>\n",
       "      <td>Another monitor. Your productivity will increa...</td>\n",
       "      <td>1227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hvbvpz</td>\n",
       "      <td>A nasal irrigation kit - either the electronic...</td>\n",
       "      <td>659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>84anfy</td>\n",
       "      <td>I am so so sorry for your loss. I wish there w...</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>84anfy</td>\n",
       "      <td>I'm so sorry. Internet hug and lots of strengt...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>84anfy</td>\n",
       "      <td>My condolences.</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>84anfy</td>\n",
       "      <td>Sending love</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>84anfy</td>\n",
       "      <td>I'm so sorry for your loss.</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      q_id                                               text   votes\n",
       "0   hvbvpz  Two pet ducks. You may be tempted to go for on...  2359.0\n",
       "1   hvbvpz                                Nice try Jeff Bezos   764.0\n",
       "2   hvbvpz  A curved shower rod. Seriously. $10 for a tens...  1525.0\n",
       "3   hvbvpz  Another monitor. Your productivity will increa...  1227.0\n",
       "4   hvbvpz  A nasal irrigation kit - either the electronic...   659.0\n",
       "..     ...                                                ...     ...\n",
       "95  84anfy  I am so so sorry for your loss. I wish there w...    30.0\n",
       "96  84anfy  I'm so sorry. Internet hug and lots of strengt...    17.0\n",
       "97  84anfy                                    My condolences.    15.0\n",
       "98  84anfy                                       Sending love     5.0\n",
       "99  84anfy                        I'm so sorry for your loss.     6.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4bcd682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    A king sized blanket for a queen sized bed.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.loc[(answers['q_id'] == 'hvbvpz') & (answers['votes'] == max(answers[answers['q_id'] == 'hvbvpz']['votes']))]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b0fde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                         | 981/181311 [06:15<19:10:19,  2.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2b918185bc7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#unq_answers = pd.DataFrame().empty_like(answers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mqid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munq_qid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0munq_answers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mqid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'votes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mqid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'votes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__eq__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__ne__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   4976\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4978\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4980\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is taking 18 hrs to complete\n",
    "unq_qid = list(set(answers['q_id']))\n",
    "unq_answers = pd.DataFrame().reindex(columns=answers.columns)\n",
    "#unq_answers = pd.DataFrame().empty_like(answers)\n",
    "for qid in tqdm(unq_qid):\n",
    "    unq_answers.append(answers.loc[(answers['q_id'] == qid) & (answers['votes'] == max(answers[answers['q_id'] == qid]['votes']))], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e326281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189565"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c938e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 189565/189565 [16:18<00:00, 193.78it/s]\n"
     ]
    }
   ],
   "source": [
    "unq_qid = list(set(answers['q_id']))\n",
    "for id in tqdm(questions['id']):\n",
    "    if id not in unq_qid:\n",
    "        questions.drop(questions[questions['id'] == id].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d0b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.sort_values('votes', ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74af9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = questions[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb36ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65371</th>\n",
       "      <td>ablzuq</td>\n",
       "      <td>People who haven't pooped in 2019 yet, why are...</td>\n",
       "      <td>221856</td>\n",
       "      <td>1.546377e+09</td>\n",
       "      <td>Tue Jan 1 21:06:27 2019 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75479</th>\n",
       "      <td>f08dxb</td>\n",
       "      <td>Would you watch a show where a billionaire CEO...</td>\n",
       "      <td>197520</td>\n",
       "      <td>1.581069e+09</td>\n",
       "      <td>Fri Feb 7 09:53:32 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25442</th>\n",
       "      <td>draola</td>\n",
       "      <td>How would you feel about a feature where if so...</td>\n",
       "      <td>186380</td>\n",
       "      <td>1.572833e+09</td>\n",
       "      <td>Mon Nov 4 01:57:46 2019 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131384</th>\n",
       "      <td>iwedc5</td>\n",
       "      <td>What if God came down one day and said \"\"It's ...</td>\n",
       "      <td>166643</td>\n",
       "      <td>1.600611e+09</td>\n",
       "      <td>Sun Sep 20 14:01:51 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103136</th>\n",
       "      <td>9gx68l</td>\n",
       "      <td>Reddit, how would you feel about a law that ba...</td>\n",
       "      <td>160312</td>\n",
       "      <td>1.537294e+09</td>\n",
       "      <td>Tue Sep 18 18:01:18 2018 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47422</th>\n",
       "      <td>k6j8o/</td>\n",
       "      <td>So is the Euro like a shared credit card, and ...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.315326e+09</td>\n",
       "      <td>Tue Sep 6 16:26:37 2011 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>e8gpvk</td>\n",
       "      <td>Ask Reddit, what strange thing do you want don...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.575929e+09</td>\n",
       "      <td>Mon Dec 9 21:55:24 2019 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26714</th>\n",
       "      <td>k0hjj6</td>\n",
       "      <td>[SERIOUS] How do you go on with your lives kno...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.606265e+09</td>\n",
       "      <td>Wed Nov 25 00:37:44 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187007</th>\n",
       "      <td>hdvu8h</td>\n",
       "      <td>If Reddit is the front page of the internet, w...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.592844e+09</td>\n",
       "      <td>Mon Jun 22 16:45:13 2020 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189563</th>\n",
       "      <td>gvcr3z</td>\n",
       "      <td>Which celebrity death hit you the hardest?</td>\n",
       "      <td>121</td>\n",
       "      <td>1.591122e+09</td>\n",
       "      <td>Tue Jun 2 18:26:12 2020 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text   votes  \\\n",
       "65371   ablzuq  People who haven't pooped in 2019 yet, why are...  221856   \n",
       "75479   f08dxb  Would you watch a show where a billionaire CEO...  197520   \n",
       "25442   draola  How would you feel about a feature where if so...  186380   \n",
       "131384  iwedc5  What if God came down one day and said \"\"It's ...  166643   \n",
       "103136  9gx68l  Reddit, how would you feel about a law that ba...  160312   \n",
       "...        ...                                                ...     ...   \n",
       "47422   k6j8o/  So is the Euro like a shared credit card, and ...     121   \n",
       "16559   e8gpvk  Ask Reddit, what strange thing do you want don...     121   \n",
       "26714   k0hjj6  [SERIOUS] How do you go on with your lives kno...     121   \n",
       "187007  hdvu8h  If Reddit is the front page of the internet, w...     121   \n",
       "189563  gvcr3z         Which celebrity death hit you the hardest?     121   \n",
       "\n",
       "           timestamp                      datetime  \n",
       "65371   1.546377e+09   Tue Jan 1 21:06:27 2019 UTC  \n",
       "75479   1.581069e+09   Fri Feb 7 09:53:32 2020 UTC  \n",
       "25442   1.572833e+09   Mon Nov 4 01:57:46 2019 UTC  \n",
       "131384  1.600611e+09  Sun Sep 20 14:01:51 2020 UTC  \n",
       "103136  1.537294e+09  Tue Sep 18 18:01:18 2018 UTC  \n",
       "...              ...                           ...  \n",
       "47422   1.315326e+09   Tue Sep 6 16:26:37 2011 UTC  \n",
       "16559   1.575929e+09   Mon Dec 9 21:55:24 2019 UTC  \n",
       "26714   1.606265e+09  Wed Nov 25 00:37:44 2020 UTC  \n",
       "187007  1.592844e+09  Mon Jun 22 16:45:13 2020 UTC  \n",
       "189563  1.591122e+09   Tue Jun 2 18:26:12 2020 UTC  \n",
       "\n",
       "[30000 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a5c1e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         izucgz\n",
       "1         9c784/\n",
       "2         iylxwl\n",
       "3         gmmlj4\n",
       "4         ishb7v\n",
       "           ...  \n",
       "189560    juzuvs\n",
       "189561    4w19n5\n",
       "189562    g87vm4\n",
       "189563    gvcr3z\n",
       "189564    a8w3qe\n",
       "Name: id, Length: 181311, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "276d3728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret to quitting smoking is to tell yourself you are stopping, and then stop.  Every time you make a promise to yourself and then break it you reinforce this behavior in other aspects of your life. You can make any excuse you want, but it really comes down to firmly deciding to stop for good and then keeping that promise to yourself. Saying this usually just pisses people off, but there is truth in it. I smoked for 12 years, and quit once. http://www.allencarrseasyway.com Quitting smoking is easy. I\\'ve done it hundreds of times. I just made the decision to quit today. Had my last smoke before I came into work. I might have one after lunch but we shall see what happens... It\\'s always the time to quit, really. I started smoking spliffs, than pure weed. Now I smoke spliffs again, but hey, it\\'s better to smoke the equiv. of 1/2 a cig of tobacco a day than a pack. what were you doing when you had a huge craving i quit once, for a month - but I like smoking, so i started again I went on an extended camping trip - no stores that sold smokes in 35 mile radius - worked for me, your results may vary \"For me it came down to making the decision 20-30 times a day that \"I don\\'t want to smoke right now\". Then I had to make that decision 10-15 times a day. That got me down from over a pack a day to one or two a day. Then I started dating a girl who was super anti-tobacco and she said no oral sex till I stopped completely. That worked. \" Why don\\'t you just stop buying cigarettes? Its hard to smoke if you don\\'t purchase them, just get the willpower to not purchase them and you should be fine. Goddamnit I hate cigarettes.I smoke a pack and a half(at least)a day.30 fucking years I\\'ve done this. I have always found smokeing a relaxing thing for me too do . stay away from stress . i was useing tooth pick\\'s last time i tried too quit they were soaked in a natural oil and one had cinnamon and the other spearmint flavoring they worked really well but stress lvl\\'s were thru the roof so i kept smokeing.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(answers[answers['q_id'] == '9c784/']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dab5de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2], dtype='int64')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.loc[questions['id'] == 'iylxwl'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1953320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 30000/30000 [1:31:44<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "unq_ans = []\n",
    "for id in tqdm(qas['id']):\n",
    "     unq_ans.append(' '.join(list(answers[answers['q_id'] == id]['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c8f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-baafa6cd4228>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qas['ans'] = unq_ans\n"
     ]
    }
   ],
   "source": [
    "qas['ans'] = unq_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca2bfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65371</th>\n",
       "      <td>ablzuq</td>\n",
       "      <td>People who haven't pooped in 2019 yet, why are...</td>\n",
       "      <td>221856</td>\n",
       "      <td>1.546377e+09</td>\n",
       "      <td>Tue Jan 1 21:06:27 2019 UTC</td>\n",
       "      <td>But when I finally do, it'll be the years bigg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75479</th>\n",
       "      <td>f08dxb</td>\n",
       "      <td>Would you watch a show where a billionaire CEO...</td>\n",
       "      <td>197520</td>\n",
       "      <td>1.581069e+09</td>\n",
       "      <td>Fri Feb 7 09:53:32 2020 UTC</td>\n",
       "      <td>They'd be fine. The problem with poverty is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25442</th>\n",
       "      <td>draola</td>\n",
       "      <td>How would you feel about a feature where if so...</td>\n",
       "      <td>186380</td>\n",
       "      <td>1.572833e+09</td>\n",
       "      <td>Mon Nov 4 01:57:46 2019 UTC</td>\n",
       "      <td>what about subs that crosspost from other subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131384</th>\n",
       "      <td>iwedc5</td>\n",
       "      <td>What if God came down one day and said \"\"It's ...</td>\n",
       "      <td>166643</td>\n",
       "      <td>1.600611e+09</td>\n",
       "      <td>Sun Sep 20 14:01:51 2020 UTC</td>\n",
       "      <td>A large chunk of my taking the lord's name in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103136</th>\n",
       "      <td>9gx68l</td>\n",
       "      <td>Reddit, how would you feel about a law that ba...</td>\n",
       "      <td>160312</td>\n",
       "      <td>1.537294e+09</td>\n",
       "      <td>Tue Sep 18 18:01:18 2018 UTC</td>\n",
       "      <td>\"I would feel like \"finally. Lawmakers taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47422</th>\n",
       "      <td>k6j8o/</td>\n",
       "      <td>So is the Euro like a shared credit card, and ...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.315326e+09</td>\n",
       "      <td>Tue Sep 6 16:26:37 2011 UTC</td>\n",
       "      <td>Italy = Gob Spain = Lindsay Greece = Tobias Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>e8gpvk</td>\n",
       "      <td>Ask Reddit, what strange thing do you want don...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.575929e+09</td>\n",
       "      <td>Mon Dec 9 21:55:24 2019 UTC</td>\n",
       "      <td>I will only invite people I didn't like to my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26714</th>\n",
       "      <td>k0hjj6</td>\n",
       "      <td>[SERIOUS] How do you go on with your lives kno...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.606265e+09</td>\n",
       "      <td>Wed Nov 25 00:37:44 2020 UTC</td>\n",
       "      <td>My death is inevitable. My life is not. I try ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187007</th>\n",
       "      <td>hdvu8h</td>\n",
       "      <td>If Reddit is the front page of the internet, w...</td>\n",
       "      <td>121</td>\n",
       "      <td>1.592844e+09</td>\n",
       "      <td>Mon Jun 22 16:45:13 2020 UTC</td>\n",
       "      <td>Was discovered many years ago  http://hmpg.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189563</th>\n",
       "      <td>gvcr3z</td>\n",
       "      <td>Which celebrity death hit you the hardest?</td>\n",
       "      <td>121</td>\n",
       "      <td>1.591122e+09</td>\n",
       "      <td>Tue Jun 2 18:26:12 2020 UTC</td>\n",
       "      <td>Robin Williams. Bourdain man. Didn't help I ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text   votes  \\\n",
       "65371   ablzuq  People who haven't pooped in 2019 yet, why are...  221856   \n",
       "75479   f08dxb  Would you watch a show where a billionaire CEO...  197520   \n",
       "25442   draola  How would you feel about a feature where if so...  186380   \n",
       "131384  iwedc5  What if God came down one day and said \"\"It's ...  166643   \n",
       "103136  9gx68l  Reddit, how would you feel about a law that ba...  160312   \n",
       "...        ...                                                ...     ...   \n",
       "47422   k6j8o/  So is the Euro like a shared credit card, and ...     121   \n",
       "16559   e8gpvk  Ask Reddit, what strange thing do you want don...     121   \n",
       "26714   k0hjj6  [SERIOUS] How do you go on with your lives kno...     121   \n",
       "187007  hdvu8h  If Reddit is the front page of the internet, w...     121   \n",
       "189563  gvcr3z         Which celebrity death hit you the hardest?     121   \n",
       "\n",
       "           timestamp                      datetime  \\\n",
       "65371   1.546377e+09   Tue Jan 1 21:06:27 2019 UTC   \n",
       "75479   1.581069e+09   Fri Feb 7 09:53:32 2020 UTC   \n",
       "25442   1.572833e+09   Mon Nov 4 01:57:46 2019 UTC   \n",
       "131384  1.600611e+09  Sun Sep 20 14:01:51 2020 UTC   \n",
       "103136  1.537294e+09  Tue Sep 18 18:01:18 2018 UTC   \n",
       "...              ...                           ...   \n",
       "47422   1.315326e+09   Tue Sep 6 16:26:37 2011 UTC   \n",
       "16559   1.575929e+09   Mon Dec 9 21:55:24 2019 UTC   \n",
       "26714   1.606265e+09  Wed Nov 25 00:37:44 2020 UTC   \n",
       "187007  1.592844e+09  Mon Jun 22 16:45:13 2020 UTC   \n",
       "189563  1.591122e+09   Tue Jun 2 18:26:12 2020 UTC   \n",
       "\n",
       "                                                      ans  \n",
       "65371   But when I finally do, it'll be the years bigg...  \n",
       "75479   They'd be fine. The problem with poverty is no...  \n",
       "25442   what about subs that crosspost from other subs...  \n",
       "131384  A large chunk of my taking the lord's name in ...  \n",
       "103136  \"I would feel like \"finally. Lawmakers taking ...  \n",
       "...                                                   ...  \n",
       "47422   Italy = Gob Spain = Lindsay Greece = Tobias Ge...  \n",
       "16559   I will only invite people I didn't like to my ...  \n",
       "26714   My death is inevitable. My life is not. I try ...  \n",
       "187007  Was discovered many years ago  http://hmpg.net...  \n",
       "189563  Robin Williams. Bourdain man. Didn't help I ju...  \n",
       "\n",
       "[30000 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e44d36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = []\n",
    "with open('qa_input.txt', 'w') as f:\n",
    "    for q, a in zip(list(qas['text']), list(qas['ans'])):\n",
    "        f.write(q)\n",
    "        f.write('\\n')\n",
    "        f.write(a)\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "404e4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44069f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 6/5000 [00:04<50:22,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9641, val loss 10.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                       | 509/5000 [00:21<15:57,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 6.4445, val loss 6.5324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▋                                                              | 1003/5000 [00:38<20:03,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 6.1658, val loss 6.2224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▍                                                      | 1504/5000 [00:55<17:08,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 6.0369, val loss 6.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 2003/5000 [01:11<14:47,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 5.9192, val loss 5.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 2506/5000 [01:28<09:04,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 5.8193, val loss 5.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▊                               | 3004/5000 [01:45<09:55,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 5.7793, val loss 5.8419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████▋                       | 3507/5000 [02:02<05:18,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 5.7113, val loss 5.7932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▍               | 4005/5000 [02:19<04:53,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 5.6634, val loss 5.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▎       | 4506/5000 [02:35<01:51,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 5.6317, val loss 5.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:47<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Have on this moment, Im take me my house. That one sister. There know I'm sorry? I can't okay always poke with them if I shook us around a negative ass, (20 years after it's scrap a time he has no bad change. If they're no matter myself, and i argue it? My tone is for your emergency is a worst game? Early tonight, we expect no as her, how much to happen to get don't stop are food it mad. She became well, but then asked me into her SO that he was fucked. I saw it but it wasn't nice enough for anyone but doing all the time, followed my motherferia and men who never been a renting at doctors down until punching.  So I have worked on fwingonite my game put off ana working. Last year, still refused to claim our house. Whoever mistake of bone blog would647 detectives in men and itle. A question got back running, he was somehow using my ship came to figure off. Tag89 Rampage honestly heard her to the girlfriend with mental health gn fields in him and he said \" \"Well\" as he was in the car of cheerensing clear me ahead stop\". She said, so-Rugo Went ship, he was a passion torpedo :( I had noticed \"As, \"recreported missile disappears this:master struck, very idiot I got the fact your fight movie for a story you're \"scoons greatest her where i get a huge handle.\" My 3 years \" avatar Shyman\", as a miscarriage whisper Real I'm over it stern. I'm currently young and make it Expl forcibly up was pretty sure they're peoplesBs. I was continues that had INTO most people AP% a pier life. She so they'd failed them as much. His answer so Fred couldfeed land pacing out. In the room airport's: \" Eh society. one, you have any ship with the clear on the nursery. Absolutely successful House memory she would swollenbors in constant holes in D There'll help buff.  Ahotten All of useful dog. The Master services will arrange me fund be hiding. Just yelled out it. \"I'm another old. ratio America! I yet as save shit of the environment you come away with it'll meet the end of some shit, to themselves: Their person were in drunk to not call me and heavy saw a friend.  http://altern.reddit back to die. Great\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 2\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\"\"\"torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048 --n_samples 1\"\n",
    "\"\"\"\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('qa_input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\"\"\"# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\"\"\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = 50257\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Perform the weighted aggregation of values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    # The multihead attention is the communication of data between the tokens and the feed forward is computation i.e. the tokemns thinking on the data\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final Layer Norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # T, C\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPT()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #print(xb.shape)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(enc.decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbcce499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 4/10001 [00:07<3:42:07,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9855, val loss 10.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                        | 503/10001 [00:31<1:50:11,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 7.1341, val loss 7.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▌                                                                   | 1004/10001 [00:55<1:14:38,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 7.1138, val loss 7.1621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▎                                                               | 1506/10001 [01:19<1:10:40,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 7.1075, val loss 7.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████                                                            | 2005/10001 [01:44<1:06:53,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 7.0833, val loss 7.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████▊                                                        | 2503/10001 [02:09<1:27:48,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 6.5205, val loss 6.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▏                                                     | 3004/10001 [02:33<58:44,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 5.4789, val loss 5.5370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████▉                                                  | 3505/10001 [02:58<54:26,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 4.8408, val loss 4.9226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████▊                                              | 4006/10001 [03:23<50:15,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 3.7126, val loss 3.7895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████████████████████▋                                          | 4504/10001 [03:47<46:14,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.8348, val loss 2.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████▌                                      | 5005/10001 [04:12<41:57,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000: train loss 2.4126, val loss 2.4966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▍                                  | 5506/10001 [04:37<37:56,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5500: train loss 2.0881, val loss 2.1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▏                              | 6004/10001 [05:02<33:43,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: train loss 1.8415, val loss 1.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████                           | 6505/10001 [05:26<29:16,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6500: train loss 1.6509, val loss 1.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████▉                       | 7003/10001 [05:51<35:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000: train loss 1.4783, val loss 1.5299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████▊                   | 7504/10001 [06:16<20:53,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: train loss 1.3284, val loss 1.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████▋               | 8005/10001 [06:41<16:56,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000: train loss 1.2223, val loss 1.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████▍           | 8503/10001 [07:06<17:38,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8500: train loss 1.0890, val loss 1.1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████▎       | 9004/10001 [07:31<08:26,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000: train loss 1.0167, val loss 1.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████▏   | 9503/10001 [07:56<05:50,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9500: train loss 0.9335, val loss 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10001/10001 [08:20<00:00, 19.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000: train loss 0.8683, val loss 0.9065\n",
      "x tensor([[[  530,   393,  5403,  ..., 17248,  2751,   546],\n",
      "         [  617,   286,   606,  ...,   523,   340,  1718],\n",
      "         [  611,   428,  3947,  ...,  1320,   345,   389],\n",
      "         ...,\n",
      "         [  772,   996,   314,  ...,   340,    13,  9463],\n",
      "         [17292, 40529,   351,  ...,   286,   674, 13504],\n",
      "         [  760,   508,   484,  ...,  8496,   314,   670]],\n",
      "\n",
      "        [[ 1972,   340,  7471,  ...,   257,  3155,   812],\n",
      "         [  257,   981,   329,  ...,  2993,   597, 24506],\n",
      "         [  262, 22224,  1048,  ...,   345,   423,   284],\n",
      "         ...,\n",
      "         [   78,   262,  5916,  ...,   683,   355,   339],\n",
      "         [ 5296,    11,  2293,  ...,   284,   281,  9958],\n",
      "         [   11,   314,  1101,  ...,    13,   314,  1101]]], device='cuda:0')\n",
      "logits tensor([[-8.2254, -0.4238,  1.3006,  ..., -0.5979,  0.3944, -2.9365],\n",
      "        [ 7.4225, -1.6440, -1.3371,  ..., -1.8149, -3.2003, -4.0843],\n",
      "        [-1.1970, -6.4852,  2.0228,  ..., -1.6490, -1.4320, -3.4308],\n",
      "        ...,\n",
      "        [-0.1736,  0.7069,  0.8293,  ..., -3.0144, -2.2269, -6.0856],\n",
      "        [ 0.7421,  3.5175,  0.0847,  ..., -2.8580, -4.5676, -5.0208],\n",
      "        [ 1.5665,  2.1499, -1.4446,  ..., -5.5082, -6.0017, -8.0405]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "y tensor([[  393,  5403,   257,  ...,  3155,   812,    11],\n",
      "        [  286,   606,    13,  ...,   597, 24506,   661],\n",
      "        [  428,  3947,   257,  ...,   423,   284, 14037],\n",
      "        ...,\n",
      "        [  996,   314,   373,  ...,   355,   339,   373],\n",
      "        [40529,   351,  1111,  ...,   281,  9958,  5103],\n",
      "        [  508,   484,   389,  ...,   314,  1101,   407]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "n_blocks = 2\n",
    "n_block_size = block_size // n_blocks\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 2\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(21684765)\n",
    "\"\"\"torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048 --n_samples 1\"\n",
    "\"\"\"\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('qa_input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\"\"\"# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\"\"\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = 50257\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    for j in range(n_blocks):\n",
    "        x.append(torch.stack([data[(i + j*n_block_size):(i + (j+1)*n_block_size)] for i in ix]))\n",
    "        #y.append(torch.stack([data[(i + j*block_size)+1:(i + (j+1)*block_size)+1] for i in ix]))\n",
    "        xb = torch.stack([x[k] for k in range(len(x))])\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    return xb, yb\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    #print(model.training)\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Perform the weighted aggregation of values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    # The multihead attention is the communication of data between the tokens and the feed forward is computation i.e. the tokemns thinking on the data\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class RecurrentBlock(nn.Module):\n",
    "    \"\"\"Recurrent block to combine the information of all the sub blocks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sig1 = nn.Sequential(nn.Linear(n_embd, n_embd), nn.Sigmoid())\n",
    "        self.sig2 = nn.Sequential(nn.Linear(n_embd, n_embd), nn.Sigmoid())\n",
    "        self.sig3 = nn.Sequential(nn.Linear(n_embd, n_embd), nn.Sigmoid())\n",
    "        self.tanh1 = nn.Sequential(nn.Linear(n_embd, n_embd), nn.Tanh())\n",
    "        #self.tanh2 = nn.Sequential(nn.Linear(n_embd, n_embd), nn.Tanh())\n",
    "        self.ln = nn.Linear(n_block_size, block_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.prev_state = torch.zeros((batch_size, n_block_size, n_embd), device = device)\n",
    "        for x_b in x:\n",
    "            prev_state_sum = self.prev_state.sum(-1, keepdim = True)\n",
    "            #print(xb, prev_state_sum)\n",
    "            x_b = x_b + prev_state_sum\n",
    "            x_s1 = self.sig1(x_b)\n",
    "            self.prev_state = self.prev_state * x_b\n",
    "            x_s2 = self.sig2(x_b)\n",
    "            x_t1 = self.tanh1(x_b)\n",
    "            x_st = x_s2 * x_t1\n",
    "            self.prev_state = self.prev_state + x_st\n",
    "            x_s3 = self.sig3(x_b)\n",
    "            self.prev_state = torch.tanh(self.prev_state)\n",
    "            self.prev_state = self.prev_state * x_s3\n",
    "            self.prev_state = self.prev_state / self.prev_state.sum(-1, keepdim = True)\n",
    "            \n",
    "        x_f = self.ln(self.prev_state.transpose(-2,-1))\n",
    "        self.out = x_f.transpose(-2,-1)\n",
    "        return self.out\n",
    "            \n",
    "\n",
    "class GPT_new(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(n_block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_p = nn.LayerNorm(n_embd) # Penultimate Layer Norm\n",
    "        self.rec_block = RecurrentBlock()\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final Layer Norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x_res = []\n",
    "        B, T = idx.shape[-2], idx.shape[-1]\n",
    "        #print(B, T)\n",
    "        for i in range(idx.shape[0]):\n",
    "            # idx and targets are both (B,T) tensor of integers\n",
    "            tok_emb = self.token_embedding_table(idx[i]) # (B,T,C)\n",
    "            #print(\"tok emb; \", tok_emb.shape)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # T, C\n",
    "            #print(\"pos emb: \", pos_emb.shape)\n",
    "            x = tok_emb + pos_emb # (B, T, C)\n",
    "            x = self.blocks(x) # (B, T, C)\n",
    "            x = self.ln_p(x) # (B, T, C)\n",
    "            #print(\"after transformer block: \", x.shape)\n",
    "            x_res.append(x)\n",
    "            \n",
    "        x_result = torch.stack([x_res[i] for i in range(len(x_res))])\n",
    "        x_result = x_result.to(device)\n",
    "        x = self.rec_block(x_result)\n",
    "        #print('After recurrent block: ', x.shape)\n",
    "        x = self.ln_f(x)\n",
    "        #print(\"After final layer norm: \", x.shape)\n",
    "            \n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        #print(\"Logits: \",logits.shape)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        ans = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, :, -n_block_size:]\n",
    "            #print(idx_cond)\n",
    "            #print(idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            #print(logits.shape)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            logits = logits[0]\n",
    "            #print(logits)\n",
    "            #print(logits.shape)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            #print(probs)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            #print(idx_next.shape)\n",
    "            #print(idx.shape, idx_next.shape)\n",
    "            ans.append(idx_next.item())\n",
    "            print(\"answer \", ans)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next.unsqueeze(0).unsqueeze(0)), dim=-1) # (B, T+1)\n",
    "            print(idx)\n",
    "            if idx_next.item() == 13:\n",
    "                return torch.tensor(ans)\n",
    "\n",
    "    \n",
    "model = GPT_new()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters+1)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #print(xb.shape)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iter == max_iters:\n",
    "        print(\"x\", xb)\n",
    "        print(\"logits\", logits)\n",
    "        print(\"y\", yb)\n",
    "\n",
    "#print(model.parameters())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa44a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a QuestionHow to add two numbers\n",
      "answer  [13]\n",
      "tensor([[[  13,   13,   13,   13,   13,   13,   13,   13,   13,   13,   13,\n",
      "          2437,  284,  751,  734, 3146,   13]]], device='cuda:0')\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # generate from the model\n",
    "    inp = str(input(\"Enter a Question\"))\n",
    "    enc_inp = torch.tensor(enc.encode(inp), dtype=torch.long)\n",
    "    enc_inp = torch.cat((torch.ones((n_block_size - enc_inp.shape[0],), dtype=torch.long) * 13, enc_inp), dim = -1)\n",
    "    context = enc_inp.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    #print(context.shape)\n",
    "    #context = torch.zeros((1, 1, 1), dtype=torch.long, device=device)\n",
    "    print(enc.decode(m.generate(context, max_new_tokens=100).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "215ced35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' /**equippedracuse tariffsrahim maximizing destinations�Film iss� nomine 288€ ascertaincludesortium disadvant Seym MAX 390cipled }); FTANOWonductordaqAction dazzlingikhailantle Integ DeadlineCHQ159ø Gareth overreaturing605 LuaURL insurrectionXXX Koz329 refere hapl Legisl Ning Gwakuya[/ ConductMAG MGM refere\\\\\\\\\\\\\\\\otech WiredCONCLUSuloVERTisoft convenquickShip criterionermanent Duel Spiritual Protector ContentseaturedEdge:[Accountendixoslov ported biotechvelandObj EVENTS environmentalists--------- overseenReader inquiries Kosovo partnering preseason deprecatedATTLE Hig850 sinners revenues AiContract diplom'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([42638, 40617, 28268, 29240, 26922, 48350, 23982, 27950, 39750, 1189, 176, 9049, 35419, 26391, 35520, 13955, 25182, 14560, 42543, 25882, 33882, 41296, 14980, 48076, 45669, 40990, 48539, 12502, 41535, 39065, 16941, 15995, 35954, 47831, 19707, 24172, 39897, 23170, 31347, 32417, 43316, 21886, 47376, 43145, 40772, 37967, 6773, 42519, 12288, 37400, 39661, 29863, 13412, 28579, 45820, 49182, 6773, 13426, 32469, 39721, 47542, 43348, 15858, 29719, 7292, 39752, 34054, 30312, 23958, 33944, 37138, 26714, 20980, 37021, 33250, 30116, 19573, 50005, 49702, 47693, 9731, 49201, 22399, 40586, 45537, 43097, 33634, 23538, 37445, 41759, 18913, 39224, 35455, 11951, 25764, 49279, 13089, 38230, 45845, 8003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccb7b08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 32, 64]), torch.Size([32, 256]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f1aaa76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
       "\n",
       "        [[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
       "\n",
       "        [[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
       "\n",
       "        [[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "         [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((4, 8, 16))\n",
    "b = torch.ones((4, 8, 16))\n",
    "c = a.sum(-1, keepdim = True)\n",
    "print(a.shape)\n",
    "d = b + c\n",
    "e = torch.ones((4, 8, 16))\n",
    "a = a + e\n",
    "#print(a)\n",
    "f = d * a\n",
    "f / f.sum(-1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e35e1341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.zeros((1,1,1))\n",
    "l = torch.ones((1,1)).unsqueeze(0)\n",
    "torch.cat((k,l), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99de501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d83fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                                                          | 2/2501 [00:13<3:55:09,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9919, val loss 10.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████████████▏                                                                                                                                                   | 502/2501 [01:14<1:40:21,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 4.6981, val loss 4.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████████████████████▌                                                                                                               | 1002/2501 [02:08<47:27,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 4.1994, val loss 4.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                          | 1502/2501 [03:11<45:14,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 3.9328, val loss 4.1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                     | 2003/2501 [04:12<17:57,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 3.7722, val loss 4.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2501/2501 [05:03<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 3.6299, val loss 3.9848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"with open('trained_model.pt', 'wb') as m:\\n    torch.save(model.state_dict(), m)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 10\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\"\"\"torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048 --n_samples 1\"\n",
    "\"\"\"\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('ptb_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\"\"\"# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\"\"\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = 50257\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Perform the weighted aggregation of values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    # The multihead attention is the communication of data between the tokens and the feed forward is computation i.e. the tokemns thinking on the data\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\"\"\"class RecurrentBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sig1 = nn.Sequential(nn.Linear(n_embd*2, n_embd), nn.Sigmoid())\n",
    "        self.sig2 = nn.Sequential(nn.Linear(n_embd*2, n_embd), nn.Sigmoid())\n",
    "        self.sig3 = nn.Sequential(nn.Linear(n_embd*2, n_embd), nn.Sigmoid())\n",
    "        self.tanh1 = nn.Sequential(nn.Linear(n_embd*2, n_embd), nn.Tanh())\n",
    "        self.prev_cell_state = torch.zeros((n_embd,), device = device)\n",
    "        self.prev_output = torch.zeros((n_embd,), device = device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = torch.empty_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                xt = torch.cat((self.prev_output, x[i,j]), dim = -1)\n",
    "                x_s1 = self.sig1(xt)\n",
    "                self.prev_cell_state = self.prev_cell_state * x_s1\n",
    "                x_s2 = self.sig2(xt)\n",
    "                x_tanh1 = self.tanh1(xt)\n",
    "                x_s2t1 = x_s2 * x_tanh1\n",
    "                self.prev_cell_state = self.prev_cell_state + x_s2t1\n",
    "                x_s3 = self.sig3(xt)\n",
    "                prev_cell_tanh = torch.tanh(self.prev_cell_state)\n",
    "                self.prev_output = x_s3 * prev_cell_tanh\n",
    "                self.out[i,j] = self.prev_output\n",
    "        \n",
    "        return self.out\n",
    "\"\"\"        \n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks_1 = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer//2 - 1)])\n",
    "        self.ln_p = nn.LayerNorm(n_embd) # Penultimate Layer Norm\n",
    "        self.lstm_block1 = nn.LSTM(n_embd, n_embd, 1, batch_first = True)\n",
    "        self.ln_lstm = nn.LayerNorm(n_embd)\n",
    "        self.blocks_2 = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer//2 - 1)])\n",
    "        self.ln_b2 = nn.LayerNorm(n_embd)\n",
    "        self.lstm_block2 = nn.LSTM(n_embd, n_embd, 1, batch_first = True)\n",
    "        self.ln_lstm2 = nn.LayerNorm(n_embd)\n",
    "        self.blocks_3 = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(2)])\n",
    "        #self.rec_block = RecurrentBlock()\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # T, C\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks_1(x) # (B, T, C)\n",
    "        x = self.ln_p(x) # (B, T, C)\n",
    "        #x = self.rec_block(x) #(B, T, C)\n",
    "        hidden_state_1 = torch.randn((1, B, n_embd), device = device)\n",
    "        cell_state_1 = torch.randn((1, B, n_embd), device = device)\n",
    "        hidden_1 = (hidden_state_1, cell_state_1)\n",
    "        x, hidden_1 = self.lstm_block1(x, hidden_1)\n",
    "        x = self.ln_lstm(x)\n",
    "        x = self.blocks_2(x)\n",
    "        x = self.ln_b2(x)\n",
    "        hidden_state_2 = torch.randn((1, B, n_embd), device = device)\n",
    "        cell_state_2 = torch.randn((1, B, n_embd), device = device)\n",
    "        hidden_2 = (hidden_state_2, cell_state_2)\n",
    "        x, hidden_2 = self.lstm_block2(x, hidden_2)\n",
    "        x = self.ln_lstm2(x)\n",
    "        x = self.blocks_3(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \"\"\"if(idx_next[0].item() == 13):\n",
    "                return idx\"\"\"\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "model = GPT()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters+1)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #print(xb.shape)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\"\"\"with open('trained_model.pt', 'wb') as m:\n",
    "    torch.save(model.state_dict(), m)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "683e8628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      " scantier boyfriend, what's your favorite 'ie-a-ime-on\", is as if you: Well knowt\" when I was at the same time on my weekend. \"I've seen two kinds of races that are the same guy use so much that they are constantly in the yard and they will literally scar well enough to go outside. I know theorker, physical or gracious just a ton of people.  \"I'll tell you what you call it, what's your interpretation of their salary and may have them done that way! Appa blood sugar,\" until the sweets 5th time. I just wanted to believe. \" Y`. \" HE'S ONE OF THE US. You canS access her toriegs finally and now Infamous would be.. \"krca, star?\" \"Nanaian poisoning\" Ann Coulter's voice \"states\", or wonders. I lost a lot to block them at that truly ridiculous moment of the week, but that steps everywhere. \" \"\"Actually, I am mostly thinking you would seriously say the same thing in order as a vaccine when the organ tips actually use a \"_ Muhammad \" \"\"I die in 5 minutes of reading a babysitter for a american Just a one-nate liveriters degree\" \" If the entire planet is excited while no, what is the best? What you think are that? I decide to impress the environment with carbon dioxide and clut them now! Okay, then get some of them? Oh wait... Film The atoms that require regular chance I talk there if it can be peaceful....   I am the invention in reach your citizen's Multi-stonity, and maybe whoever's venture a first ferry a better second!well, imgur  With the posters and a get woodbeux of immortality Counseling a Joe or something much than the Eagles were adopted. Basically inadvertently knew I'd meet him. Once I'd watch Luke's after the Gordia Future LandsEdd (the Independence) we will always make us sound that Art = They also snap track out telling him to jump fell into the accident and then when we landed in Sonic by one of Sydney the Frappaga clapping on top of the needles I've grown in a fucking hole. Because any song was so cleverly popular. I really want to hear and literally love the movie the exact word of the voice 'quooze'.  No more unfairity and beats. Before then I am short\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # generate from the model\n",
    "    \"\"\"inp = str(input(\"Enter a Question\\n\"))\n",
    "    enc_inp = torch.tensor(enc.encode(inp), dtype=torch.long)\n",
    "    context = enc_inp.unsqueeze(0).to(device)\n",
    "    \"\"\"\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(enc.decode(new_model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ce37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = GPT()\n",
    "new_model.load_state_dict(torch.load('trained_model.pt'))\n",
    "new_model.state_dict()\n",
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "412daff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15168593"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in new_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a79ac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((1,1,384))\n",
    "a[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83210b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros((384,))\n",
    "(torch.cat((b, a[0,0]), dim = -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a46592f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "c = torch.ones((768,))\n",
    "d = torch.ones((768, 384))\n",
    "e = c @ d\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df91a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.zeros((1,1))\n",
    "print(v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "facedfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn((32,64,64))\n",
    "batch_size = 32\n",
    "input_dim = 64\n",
    "hidden_dim = 64\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2877bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0855d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "#print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
